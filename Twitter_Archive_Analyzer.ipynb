{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f3b52d",
   "metadata": {},
   "source": [
    "# Twitter Archive Analyzer & CSV Converter\n",
    "\n",
    "This notebook helps you:\n",
    "\n",
    "- Upload one or more Twitter export files (the `.js` files inside your `data`/`tweet.js`-style export, or plain `.json`).\n",
    "- Parse Twitter's special `.js` format (it has a JavaScript variable assignment before the JSON).\n",
    "- Combine and clean tweets, deleted tweets, and note tweets.\n",
    "- Show basic statistics and interactive charts (with Plotly).\n",
    "- Export cleaned data to CSV and download it.\n",
    "\n",
    "Works in: GitHub Codespaces (JupyterLab), Local Jupyter, and Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeefa3e",
   "metadata": {},
   "source": [
    "## 1) Install dependencies\n",
    "\n",
    "If prompted to restart the kernel after installing `ipywidgets`, please do so. In Colab, installations are quick and typically don't require a restart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2992b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (safe to run multiple times).\n",
    "# - plotly for interactive charts\n",
    "# - pandas/numpy for analysis\n",
    "# - ipywidgets for a friendly upload UI (Jupyter)\n",
    "# - chardet helps detect file encodings\n",
    "%pip -q install plotly pandas numpy ipywidgets chardet python-dateutil\n",
    "\n",
    "# Enable ipywidgets in JupyterLab (no-op in Colab)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except Exception as e:\n",
    "    print('ipywidgets is not available yet. If using JupyterLab, you may need to restart the kernel after install.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and environment detection\n",
    "import io, os, re, json, sys, zipfile, textwrap\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from IPython.display import display, FileLink, FileLinks, Markdown\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Pick a sensible Plotly renderer\n",
    "if IN_COLAB:\n",
    "    pio.renderers.default = 'colab'\n",
    "else:\n",
    "    # Works in JupyterLab / Codespaces\n",
    "    pio.renderers.default = 'notebook_connected'\n",
    "\n",
    "print('Environment: Colab' if IN_COLAB else 'Environment: Jupyter / Codespaces')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72aa8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload files from your computer\n",
    "uploaded_files_bytes = None\n",
    "uploader = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colab: shows a file picker dialog\n",
    "    from google.colab import files as colab_files  # type: ignore\n",
    "    print('Colab: Use the dialog to select one or more .js/.json files to upload.')\n",
    "    uploaded_files_bytes = colab_files.upload()  # dict: name -> bytes\n",
    "else:\n",
    "    # Jupyter/Codespaces: use an ipywidgets file uploader\n",
    "    import ipywidgets as widgets\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept='.js,.json',\n",
    "        multiple=True,\n",
    "        description='Select .js/.json files',\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML('<b>Upload one or more Twitter export .js or .json files:</b>'),\n",
    "        uploader,\n",
    "    ]))\n",
    "    print('After selecting files, run the next cell to process them.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: (Advanced) Provide local workspace paths (e.g., from a checked-out repo)\n",
    "# Example: \n",
    "#local_paths = ['twits/tweets.js', 'twits/deleted-tweets.js']\n",
    "local_paths: List[str] = []\n",
    "local_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8976f5f",
   "metadata": {},
   "source": [
    "## 3) Parsing helpers\n",
    "\n",
    "These functions:\n",
    "\n",
    "- Detect encoding and decode bytes.\n",
    "- Strip the JavaScript wrapper (`window.YTD.* =`) used by Twitter exports.\n",
    "- Parse JSON safely with useful errors.\n",
    "- Normalize different record types (`tweet`, `deleted_tweet`, `note`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_decode(data: bytes) -> str:\n",
    "    \"\"\"Detect encoding and decode to text.\n",
    "    Falls back to UTF-8 with errors ignored.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ch = chardet.detect(data) or {}\n",
    "        enc = ch.get('encoding') or 'utf-8'\n",
    "        return data.decode(enc, errors='replace')\n",
    "    except Exception:\n",
    "        return data.decode('utf-8', errors='replace')\n",
    "\n",
    "\n",
    "def strip_js_wrapper(text: str) -> str:\n",
    "    \"\"\"Twitter exports in .js usually look like: \n",
    "    window.YTD.tweets.part0 = [ ... ];\n",
    "    We extract the JSON array/object from the first '[' or '{' to the matching last ']' or '}'.\n",
    "    \"\"\"\n",
    "    # Remove UTF-8 BOM if present\n",
    "    if text and text[0] == '\\ufeff':\n",
    "        text = text[1:]\n",
    "    stripped = text.strip()\n",
    "    # If it already looks like JSON, return as-is\n",
    "    if stripped.startswith('[') or stripped.startswith('{'):\n",
    "        return stripped\n",
    "    # Otherwise, try to find the first '[' ... last ']'\n",
    "    lb, rb = stripped.find('['), stripped.rfind(']')\n",
    "    if lb != -1 and rb != -1 and rb > lb:\n",
    "        return stripped[lb:rb+1]\n",
    "    # Fallback: try first '{' ... last '}'\n",
    "    lb, rb = stripped.find('{'), stripped.rfind('}')\n",
    "    if lb != -1 and rb != -1 and rb > lb:\n",
    "        return '[' + stripped[lb:rb+1] + ']'\n",
    "    raise ValueError('Could not locate JSON content within the .js file. Expected [ ... ] or { ... }.')\n",
    "\n",
    "\n",
    "def parse_twitter_export_bytes(data: bytes, filename: str) -> List[Dict]:\n",
    "    \"\"\"Parse a Twitter .js or .json export file into a Python list.\n",
    "    Each item is usually a dict with a single key like 'tweet', 'noteTweet', etc.\n",
    "    \"\"\"\n",
    "    text = detect_and_decode(data)\n",
    "    try:\n",
    "        core = strip_js_wrapper(text)\n",
    "        parsed = json.loads(core)\n",
    "        if isinstance(parsed, dict):\n",
    "            parsed = [parsed]\n",
    "        if not isinstance(parsed, list):\n",
    "            raise ValueError('Top-level JSON must be an array or object.')\n",
    "        return parsed\n",
    "    except json.JSONDecodeError as jde:\n",
    "        context = textwrap.shorten(text, width=200, placeholder='...')\n",
    "        raise ValueError(f'JSON parse error in {filename}: {jde}. Sample: {context}')\n",
    "\n",
    "\n",
    "def html_strip(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return re.sub(r'<[^>]+>', '', s)\n",
    "\n",
    "\n",
    "def safe_get(d: Dict, *keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if isinstance(cur, dict) and k in cur:\n",
    "            cur = cur[k]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "\n",
    "def normalize_items(raw_items: List[Dict], source_label: str) -> List[Dict]:\n",
    "    \"\"\"Flatten Twitter export records into a unified schema.\n",
    "    source_label is the filename (for provenance).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for it in raw_items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        if 'tweet' in it:\n",
    "            rec = it['tweet'] or {}\n",
    "            record_type = 'deleted_tweet' if 'deleted_at' in rec else 'tweet'\n",
    "            text_val = rec.get('full_text') or rec.get('text')\n",
    "            created = rec.get('created_at')\n",
    "            row = {\n",
    "                'record_type': record_type,\n",
    "                'source_file': source_label,\n",
    "                'id_str': str(rec.get('id_str') or rec.get('id') or ''),\n",
    "                'created_at': created,\n",
    "                'text': text_val,\n",
    "                'lang': rec.get('lang'),\n",
    "                'source': html_strip(rec.get('source')),\n",
    "                'favorite_count': rec.get('favorite_count'),\n",
    "                'retweet_count': rec.get('retweet_count'),\n",
    "                'possibly_sensitive': rec.get('possibly_sensitive'),\n",
    "                'deleted_at': rec.get('deleted_at'),\n",
    "                'in_reply_to_status_id': rec.get('in_reply_to_status_id') or rec.get('in_reply_to_status_id_str'),\n",
    "                'in_reply_to_user_id': rec.get('in_reply_to_user_id') or rec.get('in_reply_to_user_id_str'),\n",
    "                'in_reply_to_screen_name': rec.get('in_reply_to_screen_name'),\n",
    "            }\n",
    "            ents = rec.get('entities') or {}\n",
    "            row.update({\n",
    "                'hashtags_count': len(ents.get('hashtags') or []),\n",
    "                'user_mentions_count': len(ents.get('user_mentions') or []),\n",
    "                'urls_count': len(ents.get('urls') or []),\n",
    "                'media_count': len(ents.get('media') or []),\n",
    "            })\n",
    "            rows.append(row)\n",
    "        elif 'noteTweet' in it:\n",
    "            rec = it['noteTweet'] or {}\n",
    "            core = rec.get('core') or {}\n",
    "            row = {\n",
    "                'record_type': 'note',\n",
    "                'source_file': source_label,\n",
    "                'id_str': str(rec.get('noteTweetId') or ''),\n",
    "                'created_at': rec.get('createdAt') or rec.get('updatedAt'),\n",
    "                'text': core.get('text'),\n",
    "                'lang': None,\n",
    "                'source': 'Note',\n",
    "                'favorite_count': None,\n",
    "                'retweet_count': None,\n",
    "                'possibly_sensitive': None,\n",
    "                'deleted_at': None,\n",
    "                'in_reply_to_status_id': None,\n",
    "                'in_reply_to_user_id': None,\n",
    "                'in_reply_to_screen_name': None,\n",
    "                'hashtags_count': len(core.get('hashtags') or []),\n",
    "                'user_mentions_count': len(core.get('mentions') or []),\n",
    "                'urls_count': len(core.get('urls') or []),\n",
    "                'media_count': 0,\n",
    "            }\n",
    "            rows.append(row)\n",
    "        else:\n",
    "            # Unknown item type; keep as raw JSON string for debugging\n",
    "            rows.append({\n",
    "                'record_type': 'unknown',\n",
    "                'source_file': source_label,\n",
    "                'id_str': '',\n",
    "                'created_at': None,\n",
    "                'text': json.dumps(it)[:5000],\n",
    "                'lang': None,\n",
    "                'source': None,\n",
    "                'favorite_count': None,\n",
    "                'retweet_count': None,\n",
    "                'possibly_sensitive': None,\n",
    "                'deleted_at': None,\n",
    "                'in_reply_to_status_id': None,\n",
    "                'in_reply_to_user_id': None,\n",
    "                'in_reply_to_screen_name': None,\n",
    "                'hashtags_count': None,\n",
    "                'user_mentions_count': None,\n",
    "                'urls_count': None,\n",
    "                'media_count': None,\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "\n",
    "def coerce_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # created_at to datetime\n",
    "    if 'created_at' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "    # counts to numeric\n",
    "    for col in ['favorite_count', 'retweet_count', 'hashtags_count', 'user_mentions_count', 'urls_count', 'media_count']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    # id_str as string\n",
    "    if 'id_str' in df.columns:\n",
    "        df['id_str'] = df['id_str'].astype(str)\n",
    "    # text length\n",
    "    if 'text' in df.columns:\n",
    "        df['text_len'] = df['text'].fillna('').astype(str).str.len()\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize(df: pd.DataFrame) -> str:\n",
    "    lines = []\n",
    "    lines.append(f'Total records: {len(df):,}')\n",
    "    if 'record_type' in df.columns:\n",
    "        lines.append('By type:')\n",
    "        lines.extend(['  - ' + k + ': ' + str(v) for k, v in df['record_type'].value_counts().to_dict().items()])\n",
    "    if 'created_at' in df.columns and df['created_at'].notna().any():\n",
    "        lines.append(f'Date range: {df[\"created_at\"].min()} -> {df[\"created_at\"].max()}')\n",
    "    if 'lang' in df.columns:\n",
    "        top_langs = df['lang'].value_counts(dropna=True).head(5)\n",
    "        if not top_langs.empty:\n",
    "            lines.append('Top languages: ' + ', '.join([f\"{idx} ({val})\" for idx, val in top_langs.items()]))\n",
    "    if 'source' in df.columns:\n",
    "        top_src = df['source'].value_counts(dropna=True).head(5)\n",
    "        if not top_src.empty:\n",
    "            lines.append('Top sources: ' + ', '.join([f\"{idx} ({val})\" for idx, val in top_src.items()]))\n",
    "    if 'text_len' in df.columns and df['text_len'].notna().any():\n",
    "        lines.append(f\"Text length (chars): mean {df['text_len'].mean():.1f}, median {df['text_len'].median():.0f}\")\n",
    "    if 'favorite_count' in df.columns and df['favorite_count'].notna().any():\n",
    "        lines.append(f\"Favorites: mean {df['favorite_count'].mean():.2f}, max {df['favorite_count'].max()}\")\n",
    "    if 'retweet_count' in df.columns and df['retweet_count'].notna().any():\n",
    "        lines.append(f\"Retweets: mean {df['retweet_count'].mean():.2f}, max {df['retweet_count'].max()}\")\n",
    "    return '\\\\n'.join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71539e0a",
   "metadata": {},
   "source": [
    "## 4) Process the uploaded/provided files\n",
    "\n",
    "Run this cell after you selected files (or set `local_paths`).\n",
    "\n",
    "It will parse all files, combine them, show stats, and build charts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "errors = []\n",
    "file_count = 0\n",
    "\n",
    "def add_from_bytes_map(bmap: Dict[str, bytes]):\n",
    "    global all_rows, file_count\n",
    "    for name, data in (bmap or {}).items():\n",
    "        try:\n",
    "            items = parse_twitter_export_bytes(data, name)\n",
    "            rows = normalize_items(items, source_label=name)\n",
    "            all_rows.extend(rows)\n",
    "            file_count += 1\n",
    "        except Exception as e:\n",
    "            errors.append(f'{name}: {e}')\n",
    "\n",
    "def add_from_local_paths(paths: List[str]):\n",
    "    global all_rows, file_count\n",
    "    for p in paths or []:\n",
    "        try:\n",
    "            with open(p, 'rb') as f:\n",
    "                data = f.read()\n",
    "            items = parse_twitter_export_bytes(data, os.path.basename(p))\n",
    "            rows = normalize_items(items, source_label=os.path.basename(p))\n",
    "            all_rows.extend(rows)\n",
    "            file_count += 1\n",
    "        except FileNotFoundError:\n",
    "            errors.append(f'{p}: File not found')\n",
    "        except Exception as e:\n",
    "            errors.append(f'{p}: {e}')\n",
    "\n",
    "# 1) From Colab upload dialog\n",
    "if 'uploaded_files_bytes' in globals() and uploaded_files_bytes:\n",
    "    add_from_bytes_map(uploaded_files_bytes)\n",
    "\n",
    "# 2) From Jupyter/Codespaces widget\n",
    "if 'uploader' in globals() and uploader is not None and getattr(uploader, 'value', None):\n",
    "    # uploader.value is a tuple of dicts with keys: name, type, size, content\n",
    "    bytes_map = {item['name']: item['content'] for item in uploader.value}\n",
    "    add_from_bytes_map(bytes_map)\n",
    "\n",
    "# 3) From local workspace paths\n",
    "if local_paths:\n",
    "    add_from_local_paths(local_paths)\n",
    "\n",
    "if not all_rows:\n",
    "    print('No data was loaded. Please upload files in the prior cell or set local_paths, then run this cell again.')\n",
    "else:\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df = coerce_types(df)\n",
    "    print(f'Parsed {len(df):,} records from {file_count} file(s).')\n",
    "    if errors:\n",
    "        print('Some files had errors:')\n",
    "        for e in errors:\n",
    "            print(' -', e)\n",
    "    display(df.head(10))\n",
    "    print()\n",
    "    print('Summary:')\n",
    "    print(summarize(df))\n",
    "    # Keep as global for later cells\n",
    "    tweets_df = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4bc3d",
   "metadata": {},
   "source": [
    "## 5) Visualizations (Plotly)\n",
    "\n",
    "If you loaded data, this will render interactive charts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8349e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tweets_df' not in globals() or tweets_df.empty:\n",
    "    print('No data to visualize yet. Run the processing cell after uploading files.')\n",
    "else:\n",
    "    df = tweets_df.copy()\n",
    "    # Time series: monthly counts (tweets + deleted tweets + notes)\n",
    "    if 'created_at' in df.columns and df['created_at'].notna().any():\n",
    "        ts = (df.dropna(subset=['created_at'])\n",
    "                .set_index('created_at')\n",
    "                .assign(count=1)\n",
    "                .groupby([pd.Grouper(freq='M'), 'record_type'])['count']\n",
    "                .sum()\n",
    "                .reset_index())\n",
    "        fig_ts = px.line(ts, x='created_at', y='count', color='record_type',\n",
    "                         title='Monthly Record Counts by Type', markers=True)\n",
    "        fig_ts.update_layout(hovermode='x unified')\n",
    "        fig_ts.show()\n",
    "    else:\n",
    "        print('created_at not available for time series.')\n",
    "\n",
    "    # Text length distribution\n",
    "    if 'text_len' in df.columns and df['text_len'].notna().any():\n",
    "        fig_len = px.histogram(df, x='text_len', nbins=60, color='record_type',\n",
    "                               title='Distribution of Text Length (characters)')\n",
    "        fig_len.show()\n",
    "    else:\n",
    "        print('No text_len to chart.')\n",
    "\n",
    "    # Top languages\n",
    "    if 'lang' in df.columns and df['lang'].notna().any():\n",
    "        top_lang = df['lang'].value_counts().head(10).reset_index()\n",
    "        top_lang.columns = ['lang', 'count']\n",
    "        fig_lang = px.bar(top_lang, x='lang', y='count', title='Top Languages')\n",
    "        fig_lang.show()\n",
    "    else:\n",
    "        print('No language info available.')\n",
    "\n",
    "    # Top sources (client apps)\n",
    "    if 'source' in df.columns and df['source'].notna().any():\n",
    "        top_src = df['source'].value_counts().head(10).reset_index()\n",
    "        top_src.columns = ['source', 'count']\n",
    "        fig_src = px.bar(top_src, x='source', y='count', title='Top Sources (Client Apps)')\n",
    "        fig_src.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "        fig_src.show()\n",
    "    else:\n",
    "        print('No source info available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcce47d",
   "metadata": {},
   "source": [
    "## 6) Export to CSV and download\n",
    "\n",
    "- Saves CSVs under `exports/` in this environment.\n",
    "- In Colab, also triggers file downloads.\n",
    "- In Jupyter/Codespaces, clickable links are shown; you can also download from the left file browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tweets_df' not in globals() or tweets_df.empty:\n",
    "    print('Nothing to export yet.')\n",
    "else:\n",
    "    os.makedirs('exports', exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    paths = []\n",
    "\n",
    "    # All records\n",
    "    path_all = f'exports/twitter_records_{timestamp}.csv'\n",
    "    tweets_df.to_csv(path_all, index=False)\n",
    "    paths.append(path_all)\n",
    "\n",
    "    # Per type\n",
    "    for typ in sorted(tweets_df['record_type'].dropna().unique()):\n",
    "        dsub = tweets_df[tweets_df['record_type'] == typ]\n",
    "        p = f'exports/{typ}_{timestamp}.csv'\n",
    "        dsub.to_csv(p, index=False)\n",
    "        paths.append(p)\n",
    "\n",
    "    print('Saved CSV files:')\n",
    "    for p in paths:\n",
    "        print(' -', p)\n",
    "\n",
    "    # Clickable links in Jupyter/Codespaces\n",
    "    display(FileLinks('exports'))\n",
    "\n",
    "    if IN_COLAB:\n",
    "        from google.colab import files as colab_files  # type: ignore\n",
    "        for p in paths:\n",
    "            try:\n",
    "                colab_files.download(p)\n",
    "            except Exception as e:\n",
    "                print(f'Could not trigger download for {p}: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b690d3",
   "metadata": {},
   "source": [
    "## 7) Tips: Uploading/downloading via the UI\n",
    "\n",
    "- Google Colab:\n",
    "  - Upload: the `files.upload()` dialog (used above).\n",
    "  - Download: handled automatically in the export cell, or click the folder icon (Files) to right-click and download.\n",
    "- GitHub Codespaces / JupyterLab:\n",
    "  - Upload: Drag-and-drop files into the left file browser, or use the file upload widget provided above.\n",
    "  - Download: After export, use the clickable links shown above, or right-click files in the left pane and choose Download.\n",
    "- Local Jupyter: similar to Codespaces (left file browser, right-click to download).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442cf0c",
   "metadata": {},
   "source": [
    "## 8) Ideas for further analysis and features\n",
    "\n",
    "Here are some ideas you can add next (all doable with free pip packages):\n",
    "\n",
    "- Time-of-day / day-of-week posting patterns; calendar heatmaps.\n",
    "- Engagement analysis by content type (with/without media, replies vs. original tweets).\n",
    "- Hashtag and mention frequency; co-occurrence networks.\n",
    "- Text analytics: sentiment (e.g., `textblob`), keyword extraction (e.g., `rake-nltk`), topics (e.g., `scikit-learn` LDA).\n",
    "- URL expansion (resolve t.co links) and domain breakdown.\n",
    "- Media inventory: count images/videos, list top-performing media.\n",
    "- Create a Streamlit dashboard for shareable, interactive exploration of your archive.\n",
    "- Geotag analysis (if present).\n",
    "- Compare your activity before/after a specific date or event.\n",
    "\n",
    "Ethics & privacy: If sharing outputs, consider redacting personal data or mentions from private conversations.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
